{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c9efcf",
   "metadata": {},
   "source": [
    "# Adding AI with Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b0246b",
   "metadata": {},
   "source": [
    "## Navigating to the right directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a7d5ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pranshumaan'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc419779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/pranshumaan/TOSHIBA EXT\n"
     ]
    }
   ],
   "source": [
    "cd /media/pranshumaan/TOSHIBA\\ EXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20414f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/pranshumaan/TOSHIBA EXT/Dev/Langchain\n"
     ]
    }
   ],
   "source": [
    "cd Dev/Langchain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613a0fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_LangChain_Quickstart.ipynb            hugging_face_api_token.txt\r\n",
      "2_Loader_embedding_and_vector_db.ipynb  open_ai_api_key.txt\r\n",
      "Andrej_Karpathy_Resume.pdf              the-need-to-read.txt\r\n",
      "\u001b[0m\u001b[01;34mdb\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafd940",
   "metadata": {},
   "source": [
    "## Importing modules and setting up environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae6bb3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from getpass import getpass\n",
    "\n",
    "import huggingface_hub\n",
    "import langchain\n",
    "import openai\n",
    "from langchain import HuggingFaceHub, LLMChain, PromptTemplate\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    AIMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c136fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0c8bc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last updated: 2023-06-08T13:35:32.037663+05:30\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.6\n",
      "IPython version      : 8.12.0\n",
      "\n",
      "Compiler    : GCC 11.3.0\n",
      "OS          : Linux\n",
      "Release     : 6.2.6-76060206-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 12\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b2033f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('open_ai_api_key.txt', 'r') as file:\n",
    "    key = file.read().rstrip()\n",
    "os.environ[\"OPENAI_API_KEY\"] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b541c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hugging_face_api_token.txt', 'r') as file:\n",
    "    key = file.read().rstrip()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007afda4",
   "metadata": {},
   "source": [
    "## Large Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8764489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How would a 5 year old at Disneyland explain derivative pricing?\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "question = (\n",
    "    \"How would a 5 year old at Disneyland explain derivative pricing?\"\n",
    ")\n",
    "\n",
    "print(prompt.format(question=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7d962",
   "metadata": {},
   "source": [
    "### Grabbing a hugging face model: flan_alpaca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bb6330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 5 year old at Disneyland would explain derivative pricing by saying that when a company makes a product, it sells it to another company. This means that when the company makes a new product, it has to pay a higher price than the original product. This is because the new product is more\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"declare-lab/flan-alpaca-large\"\n",
    "flan = HuggingFaceHub(\n",
    "    repo_id=repo_id, model_kwargs={\"temperature\": 0, \"max_length\": 64}\n",
    ")\n",
    "\n",
    "response = flan(prompt.format(question=question))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f789a0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 5 year old at Disneyland would likely not be able to explain derivative pricing. Derivative pricing is a complex financial concept that requires a certain level of financial literacy and understanding of the markets.\n"
     ]
    }
   ],
   "source": [
    "text_davinci_003 = OpenAI(temperature=0)\n",
    "response = text_davinci_003(prompt.format(question=question))\n",
    "print(response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f5ec75",
   "metadata": {},
   "source": [
    "### Multiple Completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "201fdee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text=' A 5 year old at Disneyland would not be able to explain derivative pricing. Derivative pricing is a complex financial concept that requires a deep understanding of mathematics, economics, and finance.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=' A 5 year old at Disneyland would likely not be able to explain derivative pricing. Derivative pricing is a complex financial concept that involves understanding of calculus and other advanced mathematics.', generation_info={'finish_reason': 'stop', 'logprobs': None}), Generation(text=' A 5 year old at Disneyland would likely not be able to explain derivative pricing. Derivative pricing is a complex financial concept that requires a good understanding of financial markets and concepts.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 18, 'total_tokens': 126}, 'model_name': 'text-davinci-003'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_davinci_003 = OpenAI(temperature=0.4, n=3, best_of=3)\n",
    "response = text_davinci_003.generate([prompt.format(question=question)])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65fd290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1\n",
      "\n",
      "A 5 year old at Disneyland would not be able to explain derivative pricing. Derivative pricing is a complex financial concept that requires a deep understanding of mathematics, economics, and finance.\n",
      "\n",
      "Response 2\n",
      "\n",
      "A 5 year old at Disneyland would likely not be able to explain derivative pricing. Derivative pricing is a complex financial concept that involves understanding of calculus and other advanced mathematics.\n",
      "\n",
      "Response 3\n",
      "\n",
      "A 5 year old at Disneyland would likely not be able to explain derivative pricing. Derivative pricing is a complex financial concept that requires a good understanding of financial markets and concepts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, generation in enumerate(response.generations[0]):\n",
    "    print(f\"Response {i + 1}\\n\")\n",
    "    print(generation.text.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a095cf2",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4378e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "I grew up in a small town with a small library. The next town over had what I thought at the time was a big library, but it was actually more like my town had a tiny library, and the next one over had an actual small library. When I left to go to University, I found out what a real library looked like, and I was mesmerized. Books! Lots of books, many of them written in the current decade. My grades probably suffered from the amount of time I spent in the library reading things that didn’t directly relate to my classes. But there was one thing I found that would turn out to be life-changing: A real computer magazine. Last month, Harry McCracken pointed out that the last two widely-distributed American consumer computer magazines ceased paper publication. It is the end of an era, although honestly, it is more like a comatose patient expiring than a shocking and sudden demise.\n",
    "Dr. Dobb’s first issue was far from the slick commercial magazine it would become.\n",
    "\n",
    "Actually, before I had gone to college, I did have a subscription to Kilobaud, and I still have some copies of those. No offense to Wayne Green, but Kilobaud wasn’t that inspiring. It was more an extension of his magazine “73”, and while I enjoyed it, it didn’t get me dreaming. Dr. Dobb’s Journal — the magazine I found in the stacks of my University’s library — was tangibly different. There was an undertone of changing the world. We weren’t sure why yet, but we knew that soon, everyone would have a computer. Maybe they’d balance their checkbook or store recipes. A few people already saw the potential of digital music reproduction, although, I must admit, it was so poor at the time, I couldn’t imagine who would ever care.\n",
    "\n",
    "I say it was life-changing to discover the few issues of Dr. Dobb’s that were published back then because I would go on to contribute to Dr. Dobb’s throughout its storied history. I wrote the infamous DOS extender series, produced special issues, and, when it went mostly digital, was the embedded system blogger for them for more years than I care to admit. In fact, I have the dubious distinction of having the final blog posted; although the website has suffered enough bit rot, I’m not sure any of it has survived other than, maybe, on the Wayback machine. While I wasn’t with the magazine for its entire 38-year run, I read it for at least 35 and had some function there for about 24 of those.\n",
    "$2 bought a lot of computer magazine in 1977!\n",
    "\n",
    "Like a lot of nostalgia, you miss it, but it isn’t coming back. You can argue for or against it, but it doesn’t really matter.\n",
    "\n",
    "What I think Harry got right his thought piece, though, is that while digital distribution and access are awesome — look at Hackaday, for example — the magazine industry failed to figure out how to stay solvent. McCracken points out that substantial ad revenue fueled huge test labs and hardline investigative journalism in addition to the physical printing. Don’t forget that old-school magazines never made money from your subscriptions either, it was all in the ads. Now, it is hard to generate that kind of revenue and, therefore, to provide those kinds of services. So even if you prefer digital — I do — the big money is mostly gone and gone forever, it seems.\n",
    "Hard to imagine if the lads needed a day job\n",
    "\n",
    "Imagine a world where music is easily distributed for free. You’d hear a lot more music, and, in fact, you do today because independent artists can easily reach listeners directly. But how would the Beatles catalog be different if George, Paul, John, and Ringo needed day jobs? You’d imagine they might produce less and maybe even very different music.\n",
    "\n",
    "Then again, day jobs may be in short supply soon, too, at least the ones a bunch of young musicians might take. Order taker at a fast food restaurant? That’ll be AI soon enough. Fry cook? That’s already probably a robot arm.\n",
    "\n",
    "You have to wonder if the destruction of traditional media — TV, movie theaters, magazines, newspapers, books, and record labels — will lead to a different way to compensate creators fairly or if the jobs related to creation will just eventually dwindle to nothing but volunteers. Either way, the days of the big computer magazine is gone, just like the days you could make a living harvesting ice.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598964cd",
   "metadata": {},
   "source": [
    "Source: https://hackaday.com/2023/06/01/farewell-american-computer-magazines/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ab65f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40dbff30685434a90bc4ce0d3b75084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c61fbfd5be404ea9915b1358af823f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831d4394ca4947e69d4000026b92649a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7737a6edfea4f80b9f9f13eb2e7df95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b1d9b0919243d09513cf9db9ca3bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f27058acc2462d9bae0939e98ccd4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5eea4ebce14ec6b937046ec38522a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9515c0b7d1744c3490f9d7cd21355fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283015582ff046f6bd5e4cca965eb351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b358020e262c4e29be7bfd8f0e21557e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3825b724cc408d8bd1e4b94331edd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8084425d56cb4b5593256d5721b8c653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569224d3fec74768ba584ea17e5117fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c32daf0694e42b893df260f9dff9105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4eb08077",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = hf_embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78418344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e2ba9",
   "metadata": {},
   "source": [
    "### Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72a67713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jim and Dwight have a complex relationship throughout the series. At the beginning, they are bitter rivals, constantly pranking and undermining each other. However, as the series progresses, they develop a mutual respect and even a friendship. They still play pranks on each other, but they also have each other's backs when it counts. Jim even becomes Dwight's best man at his wedding. Overall, their relationship is a mix of competition, humor, and genuine affection.\n"
     ]
    }
   ],
   "source": [
    "chat_gpt = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "messages = [\n",
    "    SystemMessage(content=\"You were a writer on the TV show The Office.\"),\n",
    "    HumanMessage(content=\"What is the relationship between Jim and Dwight?\"),\n",
    "]\n",
    "response = chat_gpt(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3ed19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

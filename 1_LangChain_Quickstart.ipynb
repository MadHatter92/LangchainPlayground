{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37255cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fdf430d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'SYSTEMD_EXEC_PID': '3390',\n",
       "        'SSH_AUTH_SOCK': '/run/user/1000/keyring/ssh',\n",
       "        'SESSION_MANAGER': 'local/pop-os:@/tmp/.ICE-unix/3366,unix/pop-os:/tmp/.ICE-unix/3366',\n",
       "        'GNOME_TERMINAL_SCREEN': '/org/gnome/Terminal/screen/295c362b_fc05_432f_8e02_1e7d4a1ef0a6',\n",
       "        'LANG': 'en_IN.UTF-8',\n",
       "        'XDG_CURRENT_DESKTOP': 'pop:GNOME',\n",
       "        'PWD': '/home/pranshumaan',\n",
       "        'WAYLAND_DISPLAY': 'wayland-0',\n",
       "        'IM_CONFIG_PHASE': '1',\n",
       "        'QT_IM_MODULE': 'ibus',\n",
       "        'USER': 'pranshumaan',\n",
       "        'DESKTOP_SESSION': 'pop-wayland',\n",
       "        'XDG_MENU_PREFIX': 'gnome-',\n",
       "        'HOME': '/home/pranshumaan',\n",
       "        'MOZ_USE_XINPUT2': '1',\n",
       "        'DBUS_SESSION_BUS_ADDRESS': 'unix:path=/run/user/1000/bus',\n",
       "        'SSH_AGENT_LAUNCHER': 'gnome-keyring',\n",
       "        '_': '/usr/bin/gnome-session',\n",
       "        'XDG_CONFIG_DIRS': '/etc/xdg/xdg-pop-wayland:/etc/xdg',\n",
       "        'GTK_IM_MODULE': 'ibus',\n",
       "        'XDG_SESSION_DESKTOP': 'pop-wayland',\n",
       "        'QT_ACCESSIBILITY': '1',\n",
       "        'GNOME_DESKTOP_SESSION_ID': 'this-is-deprecated',\n",
       "        'GNOME_SETUP_DISPLAY': ':2',\n",
       "        'VTE_VERSION': '6800',\n",
       "        'GTK_MODULES': 'gail:atk-bridge',\n",
       "        'LOGNAME': 'pranshumaan',\n",
       "        'GNOME_TERMINAL_SERVICE': ':1.116',\n",
       "        'GNOME_SHELL_SESSION_MODE': 'pop',\n",
       "        'PATH': '/home/pranshumaan/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin:/usr/lib/cuda/bin',\n",
       "        'XMODIFIERS': '@im=ibus',\n",
       "        'SHELL': '/bin/bash',\n",
       "        'XDG_SESSION_TYPE': 'wayland',\n",
       "        'XDG_RUNTIME_DIR': '/run/user/1000',\n",
       "        'USERNAME': 'pranshumaan',\n",
       "        'COLORTERM': 'truecolor',\n",
       "        'XAUTHORITY': '/run/user/1000/.mutter-Xwaylandauth.CBPO51',\n",
       "        'XDG_DATA_DIRS': '/usr/share/pop-wayland:/home/pranshumaan/.local/share/flatpak/exports/share:/var/lib/flatpak/exports/share:/usr/local/share/:/usr/share/:/var/lib/snapd/desktop',\n",
       "        'XDG_SESSION_CLASS': 'user',\n",
       "        'TERM': 'xterm-color',\n",
       "        'GDMSESSION': 'pop-wayland',\n",
       "        'DISPLAY': ':1',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'JPY_SESSION_NAME': '/home/pranshumaan/',\n",
       "        'JPY_PARENT_PID': '9143',\n",
       "        'CLICOLOR': '1',\n",
       "        'FORCE_COLOR': '1',\n",
       "        'CLICOLOR_FORCE': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline',\n",
       "        'OPEN_API_KEY': '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',\n",
       "        'OPENAI_API_KEY': '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'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad3b1655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81a80916",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1885b75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(cache=None, verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x7f2bde39a920>, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.9, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key=None, openai_organization=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cc09071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Config',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__class_vars__',\n",
       " '__config__',\n",
       " '__custom_root_type__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__exclude_fields__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_validators__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__include_fields__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__json_encoder__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__post_root_validators__',\n",
       " '__pre_root_validators__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__schema_cache__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__try_update_forward_refs__',\n",
       " '__validators__',\n",
       " '_abc_impl',\n",
       " '_agenerate',\n",
       " '_calculate_keys',\n",
       " '_copy_and_set_values',\n",
       " '_decompose_class',\n",
       " '_default_params',\n",
       " '_enforce_dict_if_root',\n",
       " '_generate',\n",
       " '_get_value',\n",
       " '_identifying_params',\n",
       " '_init_private_attributes',\n",
       " '_invocation_params',\n",
       " '_iter',\n",
       " '_llm_type',\n",
       " 'agenerate',\n",
       " 'agenerate_prompt',\n",
       " 'batch_size',\n",
       " 'best_of',\n",
       " 'build_extra',\n",
       " 'cache',\n",
       " 'callback_manager',\n",
       " 'client',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'create_llm_result',\n",
       " 'dict',\n",
       " 'frequency_penalty',\n",
       " 'from_orm',\n",
       " 'generate',\n",
       " 'generate_prompt',\n",
       " 'get_num_tokens',\n",
       " 'get_num_tokens_from_messages',\n",
       " 'get_sub_prompts',\n",
       " 'json',\n",
       " 'logit_bias',\n",
       " 'max_retries',\n",
       " 'max_tokens',\n",
       " 'max_tokens_for_prompt',\n",
       " 'model_kwargs',\n",
       " 'model_name',\n",
       " 'modelname_to_contextsize',\n",
       " 'n',\n",
       " 'openai_api_key',\n",
       " 'openai_organization',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'prep_streaming_params',\n",
       " 'presence_penalty',\n",
       " 'request_timeout',\n",
       " 'save',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'set_callback_manager',\n",
       " 'set_verbose',\n",
       " 'stream',\n",
       " 'streaming',\n",
       " 'temperature',\n",
       " 'top_p',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'validate_environment',\n",
       " 'verbose']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50e3e937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rainbow Underwear Co.\n"
     ]
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful boxers?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d712d2",
   "metadata": {},
   "source": [
    "#### Using prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1afdd61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d832d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['product'], output_parser=None, partial_variables={}, template='What is a good name for a company that makes {product}?', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaf378f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a company that makes colorful drapes?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(product=\"colorful drapes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e57dd2d",
   "metadata": {},
   "source": [
    "#### Combining a Prompt and a LLM to make a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb1ddd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "855e4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "335f1c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nRainbow Aquariums.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Colourful fish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa7a1100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Config',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__class_vars__',\n",
       " '__config__',\n",
       " '__custom_root_type__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__exclude_fields__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_validators__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__include_fields__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__json_encoder__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__post_root_validators__',\n",
       " '__pre_root_validators__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__schema_cache__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__try_update_forward_refs__',\n",
       " '__validators__',\n",
       " '_abc_impl',\n",
       " '_acall',\n",
       " '_calculate_keys',\n",
       " '_call',\n",
       " '_chain_type',\n",
       " '_copy_and_set_values',\n",
       " '_decompose_class',\n",
       " '_enforce_dict_if_root',\n",
       " '_get_value',\n",
       " '_init_private_attributes',\n",
       " '_iter',\n",
       " '_parse_result',\n",
       " '_validate_inputs',\n",
       " '_validate_outputs',\n",
       " 'aapply',\n",
       " 'aapply_and_parse',\n",
       " 'acall',\n",
       " 'agenerate',\n",
       " 'apply',\n",
       " 'apply_and_parse',\n",
       " 'apredict',\n",
       " 'apredict_and_parse',\n",
       " 'aprep_prompts',\n",
       " 'arun',\n",
       " 'callback_manager',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'create_outputs',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'from_string',\n",
       " 'generate',\n",
       " 'input_keys',\n",
       " 'json',\n",
       " 'llm',\n",
       " 'memory',\n",
       " 'output_key',\n",
       " 'output_keys',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'predict',\n",
       " 'predict_and_parse',\n",
       " 'prep_inputs',\n",
       " 'prep_outputs',\n",
       " 'prep_prompts',\n",
       " 'prompt',\n",
       " 'run',\n",
       " 'save',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'set_callback_manager',\n",
       " 'set_verbose',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'verbose']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42dc600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c30c65",
   "metadata": {},
   "source": [
    "### Creating and Working with Agents\n",
    "Note: Need to have a SerpAPI key to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "366d8179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"0f01d326bb32db1fba37298b7f4438bc2be355e2122cb5726a9521cc4c47870b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d704d0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find the temperature first, then use the calculator to raise it to the .023 power.\n",
      "Action: Search\n",
      "Action Input: \"High temperature in SF yesterday\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mHigh: 66.2ºf @12:05 PM Low: 55.4ºf @3:35 AM Approx. Precipitation / Rain Total: in. 1hr.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now need to use the calculator to raise 66.2 to the .023 power\n",
      "Action: Calculator\n",
      "Action Input: 66.2^.023\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.1012343099196273\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.1012343099196273.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The high temperature in SF yesterday in Fahrenheit raised to the .023 power is 1.1012343099196273.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ca7d8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out what the temperatures were in the hottest and coldest places on Earth yesterday.\n",
      "Action: Search\n",
      "Action Input: \"hottest and coldest places on Earth yesterday temperature\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mOfficial weather records set for the highest and lowest temperatures plus the hottest and coldest places in the world.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to compare the temperatures of the hottest and coldest places.\n",
      "Action: Calculator\n",
      "Action Input: Highest temperature - Lowest temperature\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 66\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The temperature difference between the hottest and coldest places on Earth yesterday was 66 degrees.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The temperature difference between the hottest and coldest places on Earth yesterday was 66 degrees.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"What was the temperature difference between the hottest and coldest places on Earth yesterday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ea4be4",
   "metadata": {},
   "source": [
    "### Going from stateless to Chains and Agents with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c74b1c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hi there! It's nice to meet you. My name is AI. What's your name?\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "output = conversation.predict(input=\"Hi there!\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "600d1e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
      "Human: I'm not well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Oh no, I'm sorry to hear that. Is there anything I can do to help?\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"I'm not well! Just having a conversation with an AI.\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b4e39",
   "metadata": {},
   "source": [
    "### Langchain Chat Models\n",
    "Expose a chat like interface for an LLM under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b15e23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "499c6064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='अद्यापि अहं व्याधितो भविष्यामि यदि मध्याह्ने अहं भोजनं न कृत्वा अस्ति। (adyāpi ahaṃ vyādhitō bhaviṣyāmi yadi madhyāhne ahaṃ bhojanaṃ na kṛtvā asti.)', additional_kwargs={})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([HumanMessage(content=\"Translate this sentence from English to Sanskrit. I would feel very hungry right now if I had not had my lunch in the afternoon.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e9c112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just pointing out that the above Sanskrit is grammatically wrong while using most correct keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d50a0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='This sentence is a complex sentence with a conditional clause. The main clause is \"I would feel very hungry right now\" and the conditional clause is \"if I had not had my lunch in the afternoon.\" The verb tense used in the conditional clause is past perfect, indicating that the action of not having lunch happened before the present moment.', additional_kwargs={})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that analyzes the grammatical structures of sentences you are given.\"),\n",
    "    HumanMessage(content=\"I would feel very hungry right now if I had not had my lunch in the afternoon.\")\n",
    "]\n",
    "chat(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8579b7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text=\"J'adore la programmation.\", generation_info=None, message=AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}))], [ChatGeneration(text=\"J'adore l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'adore l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo'})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love programming.\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
    "        HumanMessage(content=\"I love artificial intelligence.\")\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6c06456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output['token_usage']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10651a",
   "metadata": {},
   "source": [
    "### Agents with Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "917cd001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use a search engine to find the score of Chennai yesterday and the current day of the year.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Chennai score yesterday\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mRead about Chennai Super Kings Cricket Team latest Scores, News, Articles only on ESPN.com. Find Team live Scores, Photos, Roster, Match updates Today.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to click on the ESPN.com link and search for the score of Chennai yesterday and the current day of the year.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"https://www.espn.com/cricket/team/_/id/336/chennai-super-kings\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mCheck ICC World Cricket League Division Three ✓ Schedule ✓ Live Cricket Score ... CSK and Punjab Kings look to bounce back from heavy defeats.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to refine my search query to find the specific information I need.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Chennai Super Kings score yesterday\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mChennai Super KingsCSK. 172/7 · GTGT. 157 (20 ov, target 173).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to use a calculator to find the difference between Chennai's score yesterday and the day of the year.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"172 - 202\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: -30\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know that Chennai scored 172 yesterday and the difference between their score and the day of the year is -30.\n",
      "\n",
      "Final Answer: Chennai scored 172 yesterday and the difference between their score and the day of the year is -30.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chennai scored 172 yesterday and the difference between their score and the day of the year is -30.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"How much did Chennai score yesterday? What was the difference between their score and the day of the year?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b984016",
   "metadata": {},
   "source": [
    "### Agents with State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19242089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"According to my sources, the weather in Bengaluru this evening is expected to be partly cloudy with a low chance of rain. The temperature is expected to be around 27-28 degrees Celsius. However, please note that weather can be unpredictable and subject to change, so it's always a good idea to check for updates closer to the time.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "conversation.predict(input=\"I wonder what the weather in Bengaluru is going to be in the evening today.\")\n",
    "\n",
    "# conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
    "\n",
    "# conversation.predict(input=\"Tell me about yourself.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4480def",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
